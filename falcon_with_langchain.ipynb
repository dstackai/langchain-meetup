{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Falcon with LangChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Falcon 7B Instruct?\n",
    "\n",
    "[Falcon-7B-Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) is a 7B parameters causal decoder-only model built by TII based on Falcon-7B and finetuned on a mixture of chat/instruct datasets. It is made available under the Apache 2.0 license."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_path = \"tiiuae/falcon-7b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dolly 2.0's instruct pipeline\n",
    "\n",
    "Even though Falcon7B-Instruct is fine-tuned on a mixture of chat/instruct datasets, if we use it as is, it won't perfectly follow instructions within a conversation. That's why we'll use [Dolly 2.0's instruct pipeline](https://huggingface.co/databricks/dolly-v2-12b/raw/main/instruct_pipeline.py) code ([`instruct_pipeline.py`](instruct_pipeline.py)) to adjust the prompt with special tokens that make the model better follow instructions within a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from instruct_pipeline import InstructionTextGenerationPipeline\n",
    "\n",
    "pipeline = InstructionTextGenerationPipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=128,\n",
    "    return_full_text=True,\n",
    "    task=\"text-generation\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain + Hugging Face Local Pipelines\n",
    "\n",
    "Now that our pipeline is ready, we will use the `HuggingFacePipeline` LangChain API to create an instance of a LangChain LLM that wraps our Falcon LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipeline)\n",
    "\n",
    "print(local_llm(\"Hi, there!\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain + Memory\n",
    "\n",
    "Now, let's utilize the LangChain feature called memory. This feature enables the model to follow instructions based on the previous conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "conversation_buff = ConversationChain(llm=local_llm, memory=ConversationBufferMemory())\n",
    "\n",
    "print(conversation_buff(\"What is the capital of England?\")[\"response\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
